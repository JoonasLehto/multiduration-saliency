{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from keras.layers import Input, TimeDistributed, Lambda, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import scipy.io\n",
    "from copy import deepcopy\n",
    "import tqdm \n",
    "import math\n",
    "import random\n",
    "\n",
    "sys.path.append('src')\n",
    "\n",
    "from data_loading import load_datasets_multiduration, load_datasets_sal_imp\n",
    "from util import get_model_by_name, get_loss_by_name\n",
    "\n",
    "# from eval import *\n",
    "# from attentive_convlstm_new import AttentiveConvLSTM2D\n",
    "# from dcn_resnet_new import dcn_resnet\n",
    "# from gaussian_prior_new import LearningPrior\n",
    "# from losses_keras2 import *\n",
    "from sal_imp_utilities import *\n",
    "from cb import InteractivePlot\n",
    "from losses_keras2 import loss_wrapper\n",
    "# #from multiduration_models import xception_3stream\n",
    "# from multiduration_models import sam_xception_timedist, sam_resnet_timedist, xception_se_lstm\n",
    "# from util import get_model_by_name\n",
    "\n",
    "# from data_loading import load_datasets_multiduration, load_datasets_sal_imp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THESE IN \n",
    "dataset = \"mit1003\"\n",
    "bp = \"/mnt/localssd2/predimportance/predimportance_shared/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [500, 3000, 5000]\n",
    "\n",
    "data = load_datasets_multiduration(dataset, times, bp=bp, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THESE IN: set training parameters \n",
    "ckpt_savedir = \"ckpt\"\n",
    "\n",
    "load_weights = False\n",
    "weightspath = \"\"\n",
    "\n",
    "batch_size = 4\n",
    "init_lr = 0.00001\n",
    "lr_reduce_by = .1\n",
    "reduce_at_epoch = 2\n",
    "n_epochs = 15\n",
    "\n",
    "opt = Adam(lr=init_lr) \n",
    "\n",
    "# losses is a dictionary mapping loss names to weights \n",
    "losses = {\n",
    "    'binary_crossentropy': 6,\n",
    "    'kl': 4,\n",
    "    'cc': -3,\n",
    "    'nss': -10\n",
    "}\n",
    "\n",
    "model_name = \"md-sem\"\n",
    "model_inp_size = (240, 320)\n",
    "model_out_size = (480, 640)\n",
    "n_timesteps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model \n",
    "n_outs_model = len(losses)\n",
    "model_params = {\n",
    "    'input_shape': model_inp_size + (3,),\n",
    "    'n_outs': n_outs_model,\n",
    "    'nb_timestep': n_timesteps\n",
    "}\n",
    "model_func, mode = get_model_by_name(model_name)\n",
    "model = model_func(**model_params)\n",
    "\n",
    "if load_weights: \n",
    "    model.load_weights(weightspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data generation and checkpoints\n",
    "if not os.path.exists(ckpt_savedir): \n",
    "    os.makedirs(ckpt_savedir)\n",
    "    \n",
    "l = []\n",
    "lw = [] \n",
    "loss_str = \"\"\n",
    "for lname, wt in losses.items():\n",
    "    l.append(get_loss_by_name(lname, model_out_size))\n",
    "    lw.append(wt)\n",
    "    loss_str += lname + str(wt)\n",
    "    \n",
    "n_output_maps_gen = len(losses)-1\n",
    "\n",
    "# Generators\n",
    "gen_train = MultidurationGenerator(\n",
    "                img_filenames=data['img_files_train'], \n",
    "                map_filenames=data['map_files_train'], \n",
    "                fix_filenames=data['fix_files_train'], \n",
    "                batch_size=batch_size, \n",
    "                mode=mode,\n",
    "                img_size=model_inp_size, \n",
    "                map_size=model_out_size,\n",
    "                shuffle=True, \n",
    "                augment=False, \n",
    "                n_output_maps=n_output_maps_gen,\n",
    "                fix_as_mat=data.get('fix_as_mat', False),\n",
    "                fix_key=data.get('fix_key', ''))\n",
    "\n",
    "gen_val = MultidurationGenerator(\n",
    "            img_filenames=data['img_files_val'], \n",
    "            map_filenames=data['map_files_val'], \n",
    "            fix_filenames=data['fix_files_val'], \n",
    "            batch_size=1, \n",
    "            mode=mode,\n",
    "            img_size=model_inp_size, \n",
    "            map_size=model_out_size,\n",
    "            shuffle=False, \n",
    "            augment=False, \n",
    "            n_output_maps=n_output_maps_gen,\n",
    "            fix_as_mat=data.get('fix_as_mat', False),\n",
    "            fix_key=data.get('fix_key', '')\n",
    "        )\n",
    "\n",
    "# Callbacks\n",
    "\n",
    "# where to save checkpoints\n",
    "filepath = os.path.join(ckpt_savedir, dataset + \"_\" + loss_str + '_ep{epoch:02d}_valloss{val_loss:.4f}.hdf5')\n",
    "print(\"Checkpoints will be saved with format %s\" % filepath)\n",
    "\n",
    "cb_chk = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, period=1)\n",
    "cb_plot = InteractivePlot()\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lrate = init_lr * math.pow(lr_reduce_by, math.floor((1+epoch)/reduce_at_epoch))\n",
    "    if epoch%reduce_at_epoch:\n",
    "        print('Reducing lr. New lr is:', lrate)\n",
    "    return lrate\n",
    "cb_sched = LearningRateScheduler(step_decay)\n",
    "\n",
    "cbs = [cb_chk, cb_sched, cb_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the generator \n",
    "img, outs = gen_train.__getitem__(1)\n",
    "print(\"batch size: %d. Num inputs: %d\" % (batch_size, len(img)))\n",
    "print(outs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=l, loss_weights=lw)\n",
    "\n",
    "print('Ready to train')\n",
    "model.fit_generator(gen_train, epochs=n_epochs, verbose=1, callbacks=cbs, validation_data=gen_val, max_queue_size=10,  workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(gen_val, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen = eval_generator(\n",
    "    img_files_val, \n",
    "    map_files_val, \n",
    "    \n",
    "    fix_files_val, \n",
    "    None, \n",
    "    inp_size=(shape_r, shape_c))\n",
    "\n",
    "examples = [next(gen) for _ in range(50)]\n",
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, maps, fixmaps, fixcoords = random.choice(examples)\n",
    "\n",
    "#images, maps = gen_val.__getitem__(np.random.randint(len(gen_val)))\n",
    "print(\"maps size\", len(maps), maps[0].shape)\n",
    "\n",
    "batch = 0\n",
    "preds = model.predict(images[0])[0][batch]\n",
    "\n",
    "times = [500, 3000, 5000]\n",
    "print(\"preds size\", preds.shape)\n",
    "n_times = len(preds)\n",
    "assert len(times) == n_times\n",
    "batch_sz = len(preds)\n",
    "copy=0\n",
    "# n_col, n_row = n_times + 2, batch_sz\n",
    "\n",
    "# plt.figure(figsize=[16,10*batch_sz])\n",
    "\n",
    "plt.imshow(reverse_preprocess(np.squeeze(images[0])))\n",
    "plt.title(\"original image %d\" % batch)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[16, 10])\n",
    "n_row=n_times\n",
    "n_col=2\n",
    "\n",
    "for time in range(n_times): \n",
    "\n",
    "#         plt.subplot(n_row, n_col, batch*n_col+1)\n",
    "#         plt.imshow(reverse_preprocess(images[batch]))\n",
    "#         plt.title('Original')\n",
    "\n",
    "    plt.subplot(n_row,n_col,time*n_col+1)\n",
    "    plt.imshow(maps[time])\n",
    "    plt.title('Gt %dms' % times[time])\n",
    "\n",
    "    plt.subplot(n_row,n_col,time*n_col+2)\n",
    "    # print(\"preds time sahpe\", preds[time].shape)\n",
    "    plt.imshow(np.squeeze(preds[time]))\n",
    "    plt.title('Prediction %dms' % times[time])\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: \n",
    "    W = \"../models/ckpt/mdsem_mit1003/mit1003_6bc4kl-3cc-10nss_ep12_valloss-28.7681.hdf5\"\n",
    "    model.load_weights(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_as_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = eval_generator(\n",
    "    img_files_val, \n",
    "    [map_files_val[0]], \n",
    "    [fix_files_val[0]], \n",
    "    None, \n",
    "    inp_size=(shape_r, shape_c),\n",
    "    fix_as_mat=fix_as_mat,\n",
    "    fix_key=fix_key, \n",
    "    fixcoord_filetype='mat'\n",
    ")\n",
    "#get_stats(model, gen, blur=True, mode='simple', n=False, imsize=(480, 620))\n",
    "get_stats_oneduration(model, gen, blur=7, mode='singlestream', start_at=None)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files_test, _, _, _, _, _, _, _ = load_datasets_multiduration('mit300', times=times, bp=bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path_test = \"/mnt/localssd2/predimportance/predimportance_shared/datasets/cat2000/testStimuli/\"\n",
    "savedir = \"../models/pred/mdsem_mit1003/mit1003_6bc4kl-3cc-10nss_ep12_valloss-28.7681/\"\n",
    "predict_and_save(model, img_files_test, inp_size=(shape_r, shape_c), savedir=savedir, blur=7, test_img_base_path=\"\", ext=\"jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_predimportance",
   "language": "python",
   "name": "venv_predimportance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
